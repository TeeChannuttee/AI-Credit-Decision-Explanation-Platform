"""
LLM Explanation Enhancer - Updated with Fine-tuned Model

Uses fine-tuned Phi-3 model to generate natural Thai explanations
for credit decisions.
"""

import sys
import os
from pathlib import Path
from typing import Dict, Optional

# Add project root to sys.path for direct script execution
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

class LLMEnhancer:
    """Generate natural explanations using fine-tuned LLM"""
    
    def __init__(self, model_path: str = "models/fine_tuned_llm/fine_tuned_model"):
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Load model if available
        if self.model_path.exists():
            self.load_model()
    
    def load_model(self):
        """Load fine-tuned model"""
        try:
            print(f"Loading fine-tuned model from {self.model_path}...")
            
            # Load base model
            base_model_name = "microsoft/Phi-3-mini-4k-instruct"
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            
            # Load LoRA adapter
            self.model = PeftModel.from_pretrained(base_model, str(self.model_path))
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
            
            print("âœ“ Fine-tuned model loaded successfully")
            
        except Exception as e:
            print(f"âš  Could not load fine-tuned model: {e}")
            print("  Falling back to template-based explanations")
            self.model = None
            self.tokenizer = None
    
    def generate_explanation(self, 
                           application: Dict,
                           decision_result: Dict,
                           max_length: int = 512) -> str:
        """
        Generate natural explanation using fine-tuned LLM
        
        Args:
            application: Application data
            decision_result: Decision result from decision engine
            max_length: Maximum length of generated text
            
        Returns:
            Natural Thai explanation
        """
        if self.model is None or self.tokenizer is None:
            return self._fallback_explanation(decision_result)
        
        try:
            # Format input
            input_text = self._format_input(application, decision_result)
            
            # Generate
            inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                print("â³ Generating AI explanation (CPU mode)... This usually takes 30-60 seconds.")
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=80,  # Reduced from 150 for much faster CPU response
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False
                )
            
            # Decode
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract output part
            if "Output:" in generated_text:
                explanation = generated_text.split("Output:")[-1].strip()
            else:
                explanation = generated_text
            
            return explanation
            
        except Exception as e:
            print(f"âš  LLM generation failed: {e}")
            return self._fallback_explanation(decision_result)
    
    def _format_input(self, application: Dict, decision_result: Dict) -> str:
        """Format input for LLM"""
        decision = decision_result['final_decision']
        ml_score = decision_result['ml_result']['ml_score']
        risk_level = decision_result['ml_result']['ml_risk_level']
        
        input_text = f"""Input: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸³à¸‚à¸­à¸ªà¸´à¸™à¹€à¸Šà¸·à¹ˆà¸­:
- à¸£à¸²à¸¢à¹„à¸”à¹‰à¸•à¹ˆà¸­à¹€à¸”à¸·à¸­à¸™: {application['monthly_income']:,} à¸šà¸²à¸—
- à¸­à¸²à¸¢à¸¸à¸‡à¸²à¸™: {application['employment_years']} à¸›à¸µ
- à¸­à¸±à¸•à¸£à¸²à¸«à¸™à¸µà¹‰à¸•à¹ˆà¸­à¸£à¸²à¸¢à¹„à¸”à¹‰: {application['debt_to_income']:.2%}
- à¸ªà¸´à¸™à¹€à¸Šà¸·à¹ˆà¸­à¹€à¸”à¸´à¸¡: {application['existing_loans']} à¸£à¸²à¸¢à¸à¸²à¸£
- à¸Šà¸³à¸£à¸°à¸¥à¹ˆà¸²à¸Šà¹‰à¸²: {application['late_payment_count']} à¸„à¸£à¸±à¹‰à¸‡
- à¸§à¸‡à¹€à¸‡à¸´à¸™à¸—à¸µà¹ˆà¸‚à¸­: {application['requested_amount']:,} à¸šà¸²à¸—
- à¹€à¸‡à¸´à¸™à¸­à¸­à¸¡: {application['savings_balance']:,} à¸šà¸²à¸—

à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™:
- à¸à¸²à¸£à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ: {decision}
- à¸„à¸°à¹à¸™à¸™ ML: {ml_score:.1%}
- à¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡: {risk_level}

Output:"""
        
        return input_text
    
    def _fallback_explanation(self, decision_result: Dict) -> str:
        """Fallback to template-based explanation"""
        from engine.explanation_engine import ExplanationEngine
        
        engine = ExplanationEngine()
        explanation = engine.generate_explanation(
            decision_result,
            {},
            language='th',
            style='formal'
        )
        
        return explanation.get('summary', 'à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢à¹„à¸”à¹‰')

# Example usage
if __name__ == "__main__":
    print("="*60)
    print("LLM ENHANCER TEST")
    print("="*60)
    
    enhancer = LLMEnhancer()
    
    if enhancer.model is not None:
        print("\nâœ“ Fine-tuned model ready!")
        print(f"  Device: {enhancer.device}")
        print(f"  Model path: {enhancer.model_path}")
        
        # Test generation
        test_app = {
            'monthly_income': 45000,
            'employment_years': 5,
            'debt_to_income': 0.35,
            'existing_loans': 2,
            'late_payment_count': 0,
            'requested_amount': 300000,
            'savings_balance': 200000
        }
        
        test_result = {
            'final_decision': 'rejected',
            'ml_result': {
                'ml_score': 0.305,
                'ml_risk_level': 'high'
            }
        }
        
        print("\nðŸ§ª Testing LLM generation...")
        explanation = enhancer.generate_explanation(test_app, test_result)
        print("\nGenerated Explanation:")
        print(explanation)
    else:
        print("\nâš  Fine-tuned model not available")
        print("  Using template-based explanations")
    
    print("\n" + "="*60)
