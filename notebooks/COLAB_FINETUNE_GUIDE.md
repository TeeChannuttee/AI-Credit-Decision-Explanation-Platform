# ü§ñ Fine-tune LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

## üìã ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°:
- ‚úÖ Google Account (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)
- ‚úÖ ‡πÑ‡∏ü‡∏•‡πå `data/llm_train.json` (400 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
- ‚úÖ ‡πÑ‡∏ü‡∏•‡πå `data/llm_test.json` (100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
- ‚úÖ ‡πÄ‡∏ß‡∏•‡∏≤ 2-3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

---

## üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥

### 1. ‡πÄ‡∏õ‡∏¥‡∏î Google Colab
1. ‡πÑ‡∏õ https://colab.research.google.com
2. ‡∏Ñ‡∏•‡∏¥‡∏Å "New Notebook"
3. **Runtime > Change runtime type > T4 GPU** (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)

### 2. Copy Code ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏õ‡∏£‡∏±‡∏ô‡πÉ‡∏ô Colab

---

## üìù CODE ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö COLAB

### Cell 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Libraries
```python
!pip install -q transformers datasets peft bitsandbytes accelerate trl
```

### Cell 2: Import ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import Dataset
import json

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
```

### Cell 3: Upload ‡πÑ‡∏ü‡∏•‡πå
```python
from google.colab import files
uploaded = files.upload()  # Upload llm_train.json
uploaded = files.upload()  # Upload llm_test.json
```

### Cell 4: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
```python
with open('llm_train.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)

print(f"‚úì Loaded {len(train_data)} training examples")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print("\nExample:")
print(train_data[0]['input'][:200])
print(train_data[0]['output'][:200])
```

### Cell 5: ‡πÇ‡∏´‡∏•‡∏î Phi-3 Model
```python
model_name = "microsoft/Phi-3-mini-4k-instruct"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("‚úì Model loaded")
```

### Cell 6: Setup LoRA
```python
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

### Cell 7: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Dataset
```python
def format_prompt(example):
    return f"""<|system|>‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£<|end|>
