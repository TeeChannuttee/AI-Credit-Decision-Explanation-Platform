{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tune Phi-3 for Thai Credit Explanations\n\nDataset: 400 train + 100 test examples\nMethod: LoRA + 4-bit quantization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers datasets peft bitsandbytes accelerate trl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload llm_train.json and llm_test.json\nfrom google.colab import files\nuploaded = files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Phi-3-mini with 4-bit quantization\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup LoRA\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["# Prepare dataset\nimport json\nfrom datasets import Dataset\n\nwith open('llm_train.json', 'r', encoding='utf-8') as f:\n    train_data = json.load(f)\n\ndef format_prompt(example):\n    return f\"\"\"<|system|>คุณเป็นผู้เชี่ยวชาญสินเชื่อ<|end|>\n