# ============================================================
# COMPLETE FINE-TUNING SCRIPT - COPY ALL TO ONE COLAB CELL
# ============================================================

# 1. Install dependencies
print("üì¶ Installing dependencies...")
get_ipython().system('pip install -q transformers datasets peft bitsandbytes accelerate trl')

# 2. Import libraries
print("\nüìö Importing libraries...")
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import Dataset
import json

print(f"‚úì PyTorch: {torch.__version__}")
print(f"‚úì CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"‚úì GPU: {torch.cuda.get_device_name(0)}")

# 3. Upload data files
print("\nüì§ Upload training files...")
from google.colab import files
print("Upload llm_train.json:")
uploaded = files.upload()
print("Upload llm_test.json:")
uploaded = files.upload()

# 4. Load dataset
print("\nüìä Loading dataset...")
with open('llm_train.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)
with open('llm_test.json', 'r', encoding='utf-8') as f:
    test_data = json.load(f)

print(f"‚úì Train: {len(train_data)} | Test: {len(test_data)}")

# 5. Load Phi-3 model
print("\nü§ñ Loading Phi-3...")
model_name = "microsoft/Phi-3-mini-4k-instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
print("‚úì Model loaded")

# 6. Setup LoRA
print("\n‚öôÔ∏è LoRA setup...")
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 7. Format dataset
print("\nüìã Formatting dataset...")
def format_prompt(ex):
    prompt = f"""<|system|>‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£<|end|>
